{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nimori/venv_envs/tf24/lib/python3.8/site-packages/fuzzywuzzy/fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from fuzzywuzzy import fuzz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp= spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOP_WORDS = spacy.lang.en.stop_words.STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat1= 'government works'\n",
    "cat2= 'healthcare and medicine'\n",
    "gov_lst= ['Additional Chief Secretary Shri Rajeshwar Singh informed that the District Program Implementation Committee has been constituted for supervision, monitoring and interdepartmental coordination of departmental schemes and activities for the various institutions run by the Tribal Regional Development Department.', 'In #Dhule district, #banrai dams are being constructed in the forest area through public participation. District Collector Sanjay Yadav has appealed to build Vanrai dams through public participation.', 'I launched 33 public awareness campaigns against Govt-19 on behalf of the Department of Public Welfare and Family Welfare to create awareness among the general public about the corona virus infection.', 'Bilaspur district has been ranked first among the best districts in the country for the work done in the last two years in the field of river drains and environment by the Ministry of Water Power, Government of India.', 'Many thanks to Leadspace Solutions Pvt Ltd for donating an ambulance to the Telangana Government under #GiftASmile initiative']\n",
    "health_lst= ['The health workers made every effort to ensure the use of public medicine against filariasis under their supervision. They deserve our cooperation and appreciation.', 'as part of its CSR initiative, handed over water coolers, computers & a medical ambulance Ambulance for Community Health Centres, Kasturba Gandhi Balika Vidyalayas & the District hospital, respectively.', 'I launched 33 public awareness campaigns against Govt-19 on behalf of the Department of Public Welfare and Family Welfare to create awareness among the general public about the corona virus infection.', 'Many thanks to Leadspace Solutions Pvt Ltd for donating an ambulance to the Telangana Government under #GiftASmile initiative', 'The first COVID-19 related mortality in Mizoram comes as a huge shock to the entire state. He was 66 years old with existing co-morbidities and was under treatment at ZMC for more than 10 days.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 5)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(gov_lst), len(health_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text(text):\n",
    "    doc = nlp(text.lower())\n",
    "    result = []\n",
    "    for token in doc:\n",
    "        if token.text in nlp.Defaults.stop_words:\n",
    "            continue\n",
    "        if token.is_punct:\n",
    "            continue\n",
    "        if token.lemma_ == '-PRON-':\n",
    "            continue\n",
    "        result.append(token.lemma_)\n",
    "    return \" \".join(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spacy Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_similarity_spacy(text1, text2):\n",
    "    base = nlp(process_text(text1))\n",
    "    compare = nlp(process_text(text2))\n",
    "    return base.similarity(compare)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fuzzy String Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_similarity_fuzzy(text1, text2):\n",
    "    str1 = nlp(process_text(text1))\n",
    "    str2 = nlp(process_text(text2))\n",
    "    return (fuzz.ratio(str1, str2))/100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fuzzy score:  0.01 \n",
      "spacy score:  0.5646434531704801\n",
      "fuzzy score:  0.08 \n",
      "spacy score:  0.6717757404080184\n",
      "fuzzy score:  0.16 \n",
      "spacy score:  0.5428499695432941\n",
      "fuzzy score:  0.16 \n",
      "spacy score:  0.6578721533985027\n",
      "fuzzy score:  0.21 \n",
      "spacy score:  0.6650230926756004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-9-61fcf1f434dd>:4: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  return base.similarity(compare)\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(gov_lst)):\n",
    "    print('fuzzy score: ', calculate_similarity_fuzzy(cat1, gov_lst[i]), '\\nspacy score: ', calculate_similarity_spacy(cat1, gov_lst[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fuzzy score:  0.28 \n",
      "spacy score:  0.8327349320379461\n",
      "fuzzy score:  0.17 \n",
      "spacy score:  0.731368962405665\n",
      "fuzzy score:  0.21 \n",
      "spacy score:  0.7219334800797961\n",
      "fuzzy score:  0.25 \n",
      "spacy score:  0.8325864868131116\n",
      "fuzzy score:  0.18 \n",
      "spacy score:  0.6922922610552803\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-9-61fcf1f434dd>:4: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  return base.similarity(compare)\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(health_lst)):\n",
    "    print('fuzzy score: ', calculate_similarity_fuzzy(cat2, health_lst[i]), '\\nspacy score: ', calculate_similarity_spacy(cat2, health_lst[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
